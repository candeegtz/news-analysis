{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ef5594-63dc-4ec7-9042-9ff2b95bf8a7",
   "metadata": {},
   "source": [
    "# Trabajo de Inteligencia artificial\n",
    " ## Análisis de noticias\n",
    "\n",
    " Realizado por:\n",
    " - Marta Aguilar Morcillo\n",
    " - Candela Jazmín Gutiérrez González\n",
    "\n",
    "Fecha: 30/05/2025\n",
    "\n",
    "Convocatoria de junio.\n",
    "\n",
    " ## 1. Lectura de datos\n",
    "\n",
    " Se comenzará con la lectura del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "664d7227-c596-4ba9-9265-2ba17abe1b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\usuario\\appdata\\roaming\\python\\python39\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\usuario\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\usuario\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ....\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "\n",
    "from nltk import download\n",
    "\n",
    "download('punkt', download_dir='.')\n",
    "download('punkt_tab')                           # Tokenización\n",
    "nltk.download('averaged_perceptron_tagger')     # POS tagging\n",
    "nltk.download('averaged_perceptron_tagger_eng') # POS tagging\n",
    "nltk.download('wordnet')                        # WordNet lemmatizer\n",
    "nltk.download('omw-1.4')                        # WordNet multilingüe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd78eedd-d4b4-4380-ada3-671d82e28eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to ....\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download('stopwords', download_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a763f51b-d254-49b1-9642-c4ef91d2e21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.data import path\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c81921b-6b6f-4e41-b7d2-ba1e3685fcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\usuario\\miniconda3\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "Requirement already satisfied: anyascii in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87eb78f2-614e-4ccb-b4de-9ed2ae89abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import re\n",
    "from bs4 import MarkupResemblesLocatorWarning\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0600f1f9-ea7b-4279-b084-e340b41f83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4734533-b325-41d9-afcc-bde22a4c36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "590d2b1f-c1bd-47f5-84e9-35b82ba8e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_vacias_ingles = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f0d9972-08dd-4181-9d8c-124eed854eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elimina_html(contenido):\n",
    "    return BeautifulSoup(contenido).get_text()\n",
    "\n",
    "def elimina_no_alfanumerico(contenido):\n",
    "    return [re.sub(r'[^\\w]', '', palabra)\n",
    "            for palabra in contenido\n",
    "            if re.search(r'\\w', palabra)]\n",
    "\n",
    "def expandir_constracciones(contenido):\n",
    "    return contractions.fix(contenido)\n",
    "\n",
    "def pasar_a_minuscula(contenido):\n",
    "    return contenido.lower()\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    texto = re.sub(r'[^a-zA-Z\\s]', ' ', texto)  # Reemplaza todo lo que no es letra o espacio con espacio\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    return texto\n",
    "\n",
    "def elimina_palabras_vacias(contenido):\n",
    "    return [palabra for palabra in contenido if palabra not in palabras_vacias_ingles]\n",
    "\n",
    "def lematizador(contenido):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos_tags = pos_tag(contenido)\n",
    "\n",
    "    resultado = []\n",
    "    for palabra, tag in pos_tags:\n",
    "        if tag.startswith('VB'):  # Verbos\n",
    "            resultado.append(lemmatizer.lemmatize(palabra, pos='v'))  # infinitivo\n",
    "        else:  # Sustantivos y el resto tal como están\n",
    "            resultado.append(palabra)\n",
    "\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be069f95-8d51-4137-82e6-c94ce64d6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proceso_contenido(texto):\n",
    "    texto = elimina_html(texto)\n",
    "    texto = expandir_constracciones(texto)\n",
    "    texto = pasar_a_minuscula(texto)\n",
    "    texto = limpiar_texto(texto)                # Limpiar antes de tokenizar\n",
    "    tokens = word_tokenize(texto)               \n",
    "    tokens = elimina_no_alfanumerico(tokens)    # Limpiar tokens individuales\n",
    "    tokens = elimina_palabras_vacias(tokens)\n",
    "    tokens = lematizador(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12d8300c-36ae-4731-ada9-25e0cf06ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar CSV con codificación segura y punto y coma como separador\n",
    "df = pd.read_csv(\"news_corpus.csv\", encoding=\"latin-1\", sep=\";\", quotechar='\"')\n",
    "\n",
    "# Verificar que tenga exactamente 3 columnas\n",
    "assert df.shape[1] == 3, \"El CSV no tiene exactamente 3 columnas. Revisa el separador o las comillas.\"\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for index, fila in df.iterrows():\n",
    "    autor = fila.iloc[0]\n",
    "    titulo = fila.iloc[1]\n",
    "    cuerpo = fila.iloc[2]\n",
    "\n",
    "    autor_proc = proceso_contenido(autor)\n",
    "    titulo_proc = proceso_contenido(titulo)\n",
    "    cuerpo_proc = proceso_contenido(cuerpo)\n",
    "\n",
    "    # Unir las tres listas en una sola lista combinada\n",
    "    fila_combinada = autor_proc + titulo_proc + cuerpo_proc\n",
    "    resultados.append(fila_combinada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec45a71-397d-4b8b-b0e4-4425fa107c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los primeros 5 documentos procesados\n",
    "for i, documento in enumerate(resultados[:5]):\n",
    "    print(f\"Documento {i+1}:\")\n",
    "    print(\" - Palabras:\", documento)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2cf2c2-a618-460d-b7fe-d1c6b435170e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
