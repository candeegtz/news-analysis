{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ef5594-63dc-4ec7-9042-9ff2b95bf8a7",
   "metadata": {},
   "source": [
    "# Trabajo de Inteligencia artificial\n",
    " ## An√°lisis de noticias\n",
    "\n",
    " Realizado por:\n",
    " - Marta Aguilar Morcillo\n",
    " - Candela Jazm√≠n Guti√©rrez Gonz√°lez\n",
    "\n",
    "Fecha: 30/05/2025\n",
    "\n",
    "Convocatoria de junio.\n",
    "\n",
    " ## 1. Lectura de datos\n",
    "\n",
    " Se comenzar√° con la lectura del corpus. Para ello, ser√° necesaria la importaci√≥n de las siguientes librer√≠as:\n",
    " - **nltk:** \n",
    " - **punkt_tab:** para la tokenizaci√≥n de las palabras de los documentos.\n",
    " - **contractions:**\n",
    " - **sklearn:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664d7227-c596-4ba9-9265-2ba17abe1b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\usuario\\appdata\\roaming\\python\\python39\\site-packages (3.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\usuario\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: click in c:\\users\\usuario\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "\n",
    "from nltk import download\n",
    "\n",
    "download('punkt_tab')                           # Tokenizaci√≥n\n",
    "nltk.download('averaged_perceptron_tagger')     # POS tagging\n",
    "nltk.download('averaged_perceptron_tagger_eng') # POS tagging\n",
    "nltk.download('wordnet')                        # WordNet lemmatizer\n",
    "nltk.download('omw-1.4')                        # WordNet multiling√ºe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a763f51b-d254-49b1-9642-c4ef91d2e21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.data import path\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c81921b-6b6f-4e41-b7d2-ba1e3685fcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\usuario\\miniconda3\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87eb78f2-614e-4ccb-b4de-9ed2ae89abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import re\n",
    "from bs4 import MarkupResemblesLocatorWarning\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0600f1f9-ea7b-4279-b084-e340b41f83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4734533-b325-41d9-afcc-bde22a4c36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "590d2b1f-c1bd-47f5-84e9-35b82ba8e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_vacias_ingles = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1465605",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f0d9972-08dd-4181-9d8c-124eed854eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elimina_html(contenido):\n",
    "    return BeautifulSoup(contenido).get_text()\n",
    "\n",
    "def elimina_no_alfanumerico(contenido):\n",
    "    return [re.sub(r'[^\\w]', '', palabra)\n",
    "            for palabra in contenido\n",
    "            if re.search(r'\\w', palabra)]\n",
    "\n",
    "def expandir_constracciones(contenido):\n",
    "    return contractions.fix(contenido)\n",
    "\n",
    "def pasar_a_minuscula(contenido):\n",
    "    return contenido.lower()\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    texto = re.sub(r'[^a-zA-Z\\s]', ' ', texto)  # Reemplaza todo lo que no es letra o espacio con espacio\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    return texto\n",
    "\n",
    "def elimina_palabras_vacias(contenido):\n",
    "    return [palabra for palabra in contenido if palabra not in palabras_vacias_ingles]\n",
    "\n",
    "def lematizador(contenido):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos_tags = pos_tag(contenido)\n",
    "\n",
    "    resultado = []\n",
    "    for palabra, tag in pos_tags:\n",
    "        if tag.startswith('VB'):  # Verbos\n",
    "            resultado.append(lemmatizer.lemmatize(palabra, pos='v'))  # infinitivo\n",
    "        else:  # Sustantivos y el resto tal como est√°n\n",
    "            resultado.append(palabra)\n",
    "\n",
    "    return resultado\n",
    "\n",
    "def extraer_noun_chunks(tokens):\n",
    "    resultados = []\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    \n",
    "    noun_chunks = [chunk.text.lower().strip() for chunk in doc.noun_chunks if len(chunk.text.split()) <= 3]\n",
    "    noun_chunks_set = set(noun_chunks)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        composed2 = \" \".join(tokens[i:i+2]).lower()\n",
    "        composed3 = \" \".join(tokens[i:i+3]).lower()\n",
    "\n",
    "        if composed3 in noun_chunks_set:\n",
    "            i += 3  \n",
    "        elif composed2 in noun_chunks_set:\n",
    "            i += 2 \n",
    "        else:\n",
    "            resultados.append(tokens[i].lower())  \n",
    "            i += 1\n",
    "\n",
    "    return resultados + noun_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be069f95-8d51-4137-82e6-c94ce64d6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proceso_contenido(texto):\n",
    "    texto = elimina_html(texto)\n",
    "    texto = expandir_constracciones(texto)\n",
    "    texto = pasar_a_minuscula(texto)\n",
    "    texto = limpiar_texto(texto)                # Limpiar antes de tokenizar\n",
    "    tokens = word_tokenize(texto)               \n",
    "    tokens = elimina_no_alfanumerico(tokens)    # Limpiar tokens individuales\n",
    "    tokens = elimina_palabras_vacias(tokens)\n",
    "    tokens = lematizador(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "12d8300c-36ae-4731-ada9-25e0cf06ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar CSV con codificaci√≥n segura y punto y coma como separador\n",
    "df = pd.read_csv(\"news_corpus.csv\", encoding=\"latin-1\", sep=\";\", quotechar='\"')\n",
    "\n",
    "# Verificar que tenga exactamente 3 columnas\n",
    "assert df.shape[1] == 3, \"El CSV no tiene exactamente 3 columnas. Revisa el separador o las comillas.\"\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for index, fila in df.iterrows():\n",
    "    autor = [fila.iloc[0]]\n",
    "    titulo = fila.iloc[1]\n",
    "    cuerpo = fila.iloc[2]   \n",
    "\n",
    "    titulo_proc = proceso_contenido(titulo)\n",
    "    cuerpo_proc = proceso_contenido(cuerpo)\n",
    "\n",
    "    # Unir las tres listas en una sola lista combinada\n",
    "    fila_combinada =  autor + titulo_proc + cuerpo_proc\n",
    "\n",
    "    contenido_final = extraer_noun_chunks(fila_combinada)\n",
    "    resultados.append(contenido_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ec45a71-397d-4b8b-b0e4-4425fa107c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento 1:\n",
      " - Palabras: ['chhavi tyagi', 'daman', 'diu', 'revoke', 'mandatory', 'rakshabandhan', 'offices', 'order', 'the', 'daman', 'diu', 'administration', 'wednesday', 'withdraw', 'circular', 'ask', 'women', 'staff', 'tie', 'rakhis', 'male', 'colleagues', 'order', 'trigger', 'backlash', 'employees', 'rip', 'apart', 'the', 'union', 'territory', 'administration', 'force', 'retreat', 'within', 'circular', 'make', 'celebrate', 'it', 'decide', 'celebrate', 'festival', 'in', 'shall', 'remain', 'collectively', 'suitable', 'time', 'wherein', 'shall', 'tie', 'rakhis', 'colleagues', 'order', 'issue', 'august', 'gurpreet', 'singh', 'deputy', 'secretary', 'personnel', 'say', 'to', 'ensure', 'one', 'skipped', 'office', 'attendance', 'report', 'send', 'government', 'next', 'even', 'the', 'two', 'notifications', 'rakshabandhan', 'leave', 'withdraw', 'mandate', 'daman', 'diu', 'administration', 'day', 'apart', 'the', 'circular', 'withdrawn', 'one', 'line', 'order', 'issue', 'late', 'evening', 'ut', 'department', 'personnel', 'administrative', 'reforms', 'the', 'circular', 'ridiculous', 'involve', 'how', 'government', 'dictate', 'i', 'tie', 'rakhi', 'we', 'maintain', 'tell', 'earlier', 'day', 'she', 'refuse', 'identify', 'former', 'gujarat', 'home', 'minister', 'praful', 'kodabhai', 'patel', 'direction', 'sources', 'say', 'rakshabandhan', 'celebration', 'bond', 'brothers', 'sisters', 'one', 'several', 'hindu', 'festivities', 'rituals', 'longer', 'confine', 'become', 'tools', 'push', 'in', 'year', 'bjp', 'storm', 'power', 'centre', 'rashtriya', 'swayamsevak', 'sangh', 'rss', 'chief', 'mohan', 'bhagwat', 'say', 'celebrate', 'widely', 'protect', 'hindu', 'culture', 'live', 'values', 'enshrine', 'the', 'rss', 'ideological', 'parent', 'ruling', 'bjp', 'last', 'year', 'modi', 'government', 'go', 'celebrate', 'a', 'year', 'cabinet', 'ministers', 'ask', 'go', 'social media', 'hours issue', 'compulsory staff', 'rakshabandhan workplace', 'it', 'rakshabandhan august', 'connection offices departments', 'open celebrate festival', 'lady staff', 'government', 'one mandate celebration', 'rakshabandhan', 'mandate', 'right issue', 'there sensitivities', 'government', 'i', 'rakhi', 'we', 'professionalism workplace official', 'hindustan times', 'she', 'the notice issue', 'daman diu administrator', 'private family affairs', 'tools', 'politic al ideologies', 'year', 'festival national significance', 'women ministers', 'government', 'border areas', 'festival soldiers', 'constituencies festival']\n",
      "\n",
      "Documento 2:\n",
      " - Palabras: ['slams', 'user', 'troll', 'divorce', 'rich', 'man', 'from', 'bollywood', 'actor', 'malaika', 'arora', 'khan', 'manage', 'make', 'debut', 'hindi', 'film', 'industry', 'blockbuster', 'debut', 'opposite', 'shah', 'rukh', 'khan', 'chaiyya', 'chaiyya', 'dil', 'se', 'still', 'remember', 'song', 'however', 'trolls', 'woman', 'first', 'matters', 'right', 'divorce', 'rich', 'man', 'on', 'share', 'follower', 'decide', 'read', 'wear', 'go', 'gym', 'salon', 'little', 'know', 'munni', 'badnam', 'star', 'would', 'reply', 'perfect', 'comeback', 'take', 'look', 'interaction', 'super', 'excite', 'affiliated', 'khanna', 'jewellers', 'khannajewellerskj', 'brand', 'ambassador', 'crafted', 'perfection', 'stun', 'statement', 'jewellery', 'must', 'khannajewellers', 'maksquad', 'hair', 'hairbypriyanka', 'stylist', 'manekaharisinghani', 'manager', 'ektakauroberoi', 'mua', 'subbu', 'photographer', 'prasdnaik', 'malaika', 'arora', 'khan', 'malaikaarorakhanofficial', 'aug', 'pdt', 'then', 'malaika', 'decide', 'reply', 'prove', 'matter', 'woman', 'successful', 'attack', 'moment', 'decides', 'decide', 'apart', 'malaika', 'literally', 'play', 'roles', 'traditionally', 'prescribe', 'woman', 'marry', 'raise', 'always', 'around', 'khandan', 'but', 'get', 'thrown', 'know', 'handle', 'divorce', 'but', 'need', 'alimony', 'buy', 'clothes', 'go', 'vacations', 'enjoy', 'life', 'if', 'anything', 'successful', 'what', 'happen', 'arbaaz', 'malaika', 'personal', 'concern', 'but', 'claim', 'malaika', 'marry', 'hold', 'water', 'for', 'agree', 'please', 'get', 'course', 'playlist', 'follow', 'daisy mowke malaika', 'special numbers', 'tv appearances', 'carve identity', 'the actor', 'song', 'woman', 'wednesday malaika arora', 'gorgeous picture instagram', 'follower', 'troll use alumni', 'alimony money', 'short clothes', 'interaction', 'super', 'every jewellery lover', 'a post share', 'malaika', 'reply', 'the entire conversation', 'step bounds society', 'successful woman', 'live life terms', 'malaika', 'roles', 'woman', 'quite early son', 'khandan', 'divorce alimony taunt', 'the details alimony', 'malaika husband', 'arbaaz khan', 'perhaps family', 'the couple', 'divorce', 'utmost dignity', 'vouch fact', 'alimony', 'clothes', 'short choice', 'vacations', 'life', 'anything', 'ex husband', 'what', 'malaika', 'divorce arbaaz money', 'water', 'agree', 'course', 'feminism others', 'popular songs', 'htshowbiz']\n",
      "\n",
      "Documento 3:\n",
      " - Palabras: ['correct', 'the', 'indira', 'gandhi', 'institute', 'medical', 'sciences', 'igims', 'patna', 'amend', 'thursday', 'replacing', 'word', 'virgin', 'unmarried', 'controversy', 'until', 'super', 'specialty', 'medical', 'institute', 'state', 'capital', 'require', 'declare', 'bachelors', 'widowers', 'virgins', 'igims', 'medical', 'superintendent', 'dr', 'manish', 'mandal', 'say', 'institute', 'director', 'dr', 'nr', 'biswas', 'hold', 'meet', 'thursday', 'morning', 'directing', 'word', 'virgin', 'immediately', 'replace', 'return', 'four', 'day', 'leave', 'absence', 'earlier', 'bihar', 'health', 'minister', 'mangal', 'pandey', 'end', 'redefine', 'justify', 'awkward', 'phrase', 'question', 'form', 'following', 'wednesday', 'minister', 'tell', 'nothing', 'wrong', 'use', 'word', 'virgin', 'simply', 'meant', 'kanya', 'kunwari', 'mean', 'unmarried', 'girl', 'pandey', 'join', 'cabinet', 'three', 'days', 'ago', 'sources', 'say', 'also', 'take', 'ask', 'it', 'even', 'ask', 'question', 'introduce', 'in', 'response', 'management', 'autonomous', 'super', 'specialty', 'health', 'facility', 'clarify', 'wednesday', 'adherence', 'central', 'civil', 'services', 'rules', 'follow', 'all', 'india', 'institute', 'medical', 'sciences', 'new', 'delhi', 'the', 'previous', 'version', 'purportedly', 'ask', 'virgins', 'ht', 'photo', 'the', 'existence', 'since', 'blame', 'faux', 'pas', 'poor', 'translation', 'part', 'individuals', 'draft', 'document', 'form', 'it', 'seek', 'know', 'employees', 'marital', 'status', 'dues', 'could', 'settle', 'basis', 'declaration', 'event', 'death', 'service', 'say', 'arshiya chopra virgin', 'unmarried igims form', 'marital declaration form', 'new recruits', 'bachelors', 'virgins', 'unmarried dr biswas', 'return', 'end', 'mean virginity attempts', 'public furore document', 'news channels', 'nothing', 'sources', 'chief minister office', 'cognizance issue', 'copy form', 'it', 'question', 'first place', 'adherence', 'new recruits', 'inception institute', 'some officials', 'document', 'the word', 'virgin mention', 'nothing virginity employee', 'it', 'dr mandal']\n",
      "\n",
      "Documento 4:\n",
      " - Palabras: ['sumedha sehra', 'aaj', 'aapne', 'pakad', 'liya', 'let', 'man', 'dujana', 'kill', 'lashkar', 'e', 'taiba', 'kashmir', 'commander', 'abu', 'dujana', 'kill', 'encounter', 'village', 'pulwama', 'district', 'earlier', 'week', 'dujana', 'manage', 'give', 'slip', 'several', 'times', 'past', 'carry', 'bounty', 'rs', 'lakh', 'head', 'reports', 'say', 'dujana', 'come', 'meet', 'inside', 'involve', 'encounter', 'try', 'dujana', 'surrender', 'refuse', 'reports', 'say', 'according', 'reports', 'dujana', 'reject', 'call', 'surrender', 'army', 'officer', 'local', 'start', 'dujana', 'after', 'initiate', 'talk', 'local', 'villager', 'hand', 'phone', 'army', 'officer', 'kya', 'maine', 'kaha', 'kya', 'how', 'i', 'ask', 'dujana', 'hear', 'ask', 'officer', 'reply', 'nahi', 'kar', 'deta', 'tu', 'galat', 'kar', 'rha', 'hai', 'why', 'surrender', 'you', 'marry', 'girl', 'what', 'right', 'when', 'told', 'use', 'pawn', 'dujana', 'sound', 'calm', 'unperturbed', 'situation', 'say', 'hum', 'nikley', 'shaheed', 'hone', 'main', 'kya', 'karu', 'jisko', 'game', 'khelna', 'hai', 'khelo', 'kabhi', 'hum', 'aage', 'kabhi', 'aap', 'aaj', 'aapne', 'pakad', 'liya', 'mubarak', 'ho', 'aapko', 'jisko', 'jo', 'karna', 'hai', 'karlo', 'i', 'leave', 'what', 'i', 'today', 'caught', 'congratulations', 'surrender', 'nahi', 'kar', 'sakta', 'jo', 'meri', 'kismat', 'may', 'likha', 'hoga', 'allah', 'wahi', 'karega', 'theek', 'hai', 'i', 'surrender', 'allaah', 'would', 'whatever', 'fate', 'dujana', 'go', 'say', 'dujana', 'belong', 'pakistan', 'lashkar', 'e', 'taiba', 'divisional', 'commander', 'south', 'kashmir', 'he', 'among', 'identify', 'with', 'head', 'a', 'terrorist', 'top', 'grade', 'also', 'give', 'burhan', 'wani', 'receive', 'inputs', 'last', 'days', 'frequent', 'houses', 'wife', 'rukaiya', 'girlfriend', 'shazia', 'police', 'keep', 'watch', 'houses', 'confirm', 'present', 'wife', 'house', 'move', 'trap', 'also', 'read', 'after', 'abu', 'dujana', 'prepare', 'want', 'terroristsabu', 'dujana', 'encounter', 'turn', 'lead', 'let', 'commander', 'dujana', 'jammu kashmir', 'security forces', 'reports', 'dujana', 'wife trap', 'house hakripora village', 'security officials', 'encounter', 'best convince', 'reports', 'dujana', 'the army commission', 'telephonic conversation', 'dujana', 'haal hai', 'i', 'dujana', 'officer', 'the officer', 'humara haal chhor', 'dujana surrender kyun', 'you', 'girl', 'pakistani agencies', 'i', 'home martyrdom', 'what', 'i', 'hai', 'i', 'allaah', 'whatever', 'dujana', 'dujana', 'top terrorists', 'indian army', 'jammu kashmir', 'rs lakh bounty', 'dujana label', 'inputs', 'houses', 'trap', 'new hitlist', 'terroristsabu', 'dujana', 'jilted lover', 'police informer', 'security forces']\n",
      "\n",
      "Documento 5:\n",
      " - Palabras: ['aarushi maheshwari', 'get', 'training', 'sex', 'trafficking', 'hotels', 'mumbai', 'indian', 'cities', 'train', 'staff', 'sex', 'traffic', 'frequent', 'requests', 'bed', 'linen', 'changes', 'left', 'door', 'days', 'end', 'behind', 'initiative', 'also', 'develop', 'mobile', 'phone', 'app', 'rescue', 'me', 'use', 'alert', 'local', 'police', 'senior', 'anti', 'trafficking', 'officers', 'see', 'suspicious', 'behavior', 'hotels', 'breed', 'grounds', 'say', 'sanee', 'awsarmmel', 'chairman', 'alumni', 'group', 'maharashtra', 'state', 'institute', 'hotel', 'management', 'catering', 'technology', 'we', 'work', 'hotels', 'across', 'country', 'we', 'commit', 'the', 'initiative', 'spearhead', 'alumni', 'group', 'back', 'come', 'amid', 'grow', 'international', 'recognition', 'hotels', 'play', 'fight', 'modern', 'day', 'slavery', 'maharashtra', 'major', 'destination', 'for', 'trafficked', 'girls', 'maharashtra', 'mumbai', 'capital', 'major', 'destination', 'traffic', 'girls', 'lure', 'poor', 'states', 'nearby', 'countries', 'promise', 'jobs', 'sell', 'domestic', 'servitude', 'with', 'traditional', 'red', 'light', 'districts', 'like', 'mumbai', 'start', 'disappear', 'push', 'underground', 'private', 'lodges', 'hotels', 'make', 'hard', 'police', 'monitor', 'awsarmmel', 'say', 'hotels', 'would', 'tell', 'need', 'watch', 'these', 'include', 'view', 'car', 'park', 'favor', 'traffickers', 'allow', 'signs', 'trouble', 'check', 'cars', 'gauge', 'say', 'often', 'notice', 'strange', 'behavior', 'girl', 'reticence', 'check', 'process', 'dependence', 'person', 'accompany', 'provide', 'but', 'cases', 'staff', 'ignore', 'tell', 'thomson', 'reuters', 'foundation', 'rescue', 'me', 'app', 'the', 'rescue', 'me', 'app', 'launched', 'couple', 'months', 'text', 'feature', 'fill', 'details', 'include', 'send', 'alert', 'police', 'human', 'trafficking', 'world', 'fastest', 'grow', 'criminal', 'enterprise', 'worth', 'estimate', 'billion', 'year', 'accord', 'international', 'labor', 'organization', 'say', 'globally', 'victims', 'force', 'last', 'year', 'include', 'hilton', 'shiva', 'hotels', 'pledge', 'examine', 'supply', 'chains', 'force', 'labor', 'train', 'staff', 'spot', 'report', 'signs', 'traffic', 'earlier', 'year', 'also', 'launch', 'initiative', 'train', 'traffic', 'vijaya', 'rahatkar', 'chairwoman', 'maharashtra', 'state', 'women', 'commission', 'say', 'initiative', 'would', 'impact', 'beyond', 'contact', 'million', 'small', 'hotels', 'across', 'india', 'also', 'develop', 'train', 'module', 'traffic', 'hospitality', 'students', 'could', 'use', 'across', 'country', 'also', 'readfyi', 'legal', 'revenge', 'child', 'sex', 'traffic', 'survivors', 'get', 'battlesmumbai', 'woman', 'dj', 'arrest', 'high', 'profile', 'sex', 'racket', 'case', 'spot signs', 'do disturb sign', 'door', 'the group', 'initiative', 'human trade', 'we', 'hospitality professionals', 'hotels', 'country', 'we', 'maharashtra state government', 'key role', 'jobs', 'rise property prices', 'mumbai', 'push', 'sex trade', 'hotels', 'signs staff', 'these', 'requests rooms', 'vet clients', 'trouble', 'cars', 'much charge awsarmmel', 'hotel staff', 'answer questions', 'proof identity', 'cases', 'staff', 'signs idea', 'room numbers', 'world', 'nearly million people', 'victims', 'labor trafficking', 'major hotel groups', 'traffic', 'mexico city', 'initiative', 'state alumni group', 'india', 'the group', 'country', 'school justice fight']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar los primeros 5 documentos procesados\n",
    "for i, documento in enumerate(resultados[:5]):\n",
    "    print(f\"Documento {i+1}:\")\n",
    "    print(\" - Palabras:\", documento)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb2cf2c2-a618-460d-b7fe-d1c6b435170e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü© Tema 1:\n",
      "   Palabras clave iniciales: ['child', 'family', 'welfare']\n",
      "   ‚ûï Palabras a√±adidas por TF-IDF: ['alimony', 'celebrate', 'circular', 'daman', 'diu', 'edit', 'embryos', 'festival', 'gene', 'hotel', 'hotels', 'malaika', 'nota', 'option', 'party', 'rakshabandhan', 'scientists', 'sex', 'signs', 'staff']\n",
      "   ‚úÖ Tema final: ['alimony', 'celebrate', 'child', 'circular', 'daman', 'diu', 'edit', 'embryos', 'family', 'festival', 'gene', 'hotel', 'hotels', 'malaika', 'nota', 'option', 'party', 'rakshabandhan', 'scientists', 'sex', 'signs', 'staff', 'welfare']\n",
      "\n",
      "üü© Tema 2:\n",
      "   Palabras clave iniciales: ['science', 'space', 'technology']\n",
      "   ‚ûï Palabras a√±adidas por TF-IDF: ['abuse', 'accident', 'athlete', 'buildings', 'court', 'cpwd', 'delhi', 'hussain', 'kumar', 'municipal', 'negligence', 'police', 'quote', 'say', 'snowshoe', 'station', 'suspect', 'unsafe', 'vehicle', 'woman']\n",
      "   ‚úÖ Tema final: ['abuse', 'accident', 'athlete', 'buildings', 'court', 'cpwd', 'delhi', 'hussain', 'kumar', 'municipal', 'negligence', 'police', 'quote', 'say', 'science', 'snowshoe', 'space', 'station', 'suspect', 'technology', 'unsafe', 'vehicle', 'woman']\n",
      "\n",
      "üü© Tema 3:\n",
      "   Palabras clave iniciales: ['match', 'score', 'sports']\n",
      "   ‚ûï Palabras a√±adidas por TF-IDF: ['abuse', 'air', 'athlete', 'dujana', 'girl', 'hai', 'hussain', 'korea', 'missile', 'north', 'particulate', 'pm', 'programme', 'snowshoe', 'sprague', 'study', 'surrender', 'trump', 'us', 'war']\n",
      "   ‚úÖ Tema final: ['abuse', 'air', 'athlete', 'dujana', 'girl', 'hai', 'hussain', 'korea', 'match', 'missile', 'north', 'particulate', 'pm', 'programme', 'score', 'snowshoe', 'sports', 'sprague', 'study', 'surrender', 'trump', 'us', 'war']\n",
      "\n",
      "üü© Tema 4:\n",
      "   Palabras clave iniciales: ['opinion', 'public', 'speech']\n",
      "   ‚ûï Palabras a√±adidas por TF-IDF: ['bank', 'bharti', 'congress', 'edit', 'embryos', 'gene', 'genetic', 'get', 'harshit', 'human', 'letter', 'lucknow', 'offer', 'principal', 'school', 'scientists', 'state', 'teacher', 'tomato', 'tomatoes']\n",
      "   ‚úÖ Tema final: ['bank', 'bharti', 'congress', 'edit', 'embryos', 'gene', 'genetic', 'get', 'harshit', 'human', 'letter', 'lucknow', 'offer', 'opinion', 'principal', 'public', 'school', 'scientists', 'speech', 'state', 'teacher', 'tomato', 'tomatoes']\n",
      "\n",
      "üü© Tema 5:\n",
      "   Palabras clave iniciales: ['art', 'awards', 'culture']\n",
      "   ‚ûï Palabras a√±adidas por TF-IDF: ['baby', 'edit', 'embryos', 'foetus', 'gene', 'hand', 'hotel', 'hotels', 'human', 'rowling', 'scientists', 'sex', 'shake', 'signs', 'staff', 'thane', 'traffic', 'trump', 'tweets', 'twin']\n",
      "   ‚úÖ Tema final: ['art', 'awards', 'baby', 'culture', 'edit', 'embryos', 'foetus', 'gene', 'hand', 'hotel', 'hotels', 'human', 'rowling', 'scientists', 'sex', 'shake', 'signs', 'staff', 'thane', 'traffic', 'trump', 'tweets', 'twin']\n",
      "\n",
      "üü© Tema 6:\n",
      "   Palabras clave iniciales: ['infrastructure', 'traffic', 'transport', 'urbanism']\n",
      "   ‚ûï Palabras a√±adidas por TF-IDF: ['celebrate', 'circular', 'daman', 'diu', 'festival', 'free', 'gulalai', 'haj', 'house', 'khan', 'mall', 'mcg', 'park', 'parking', 'police', 'property', 'pti', 'rakshabandhan', 'tax', 'women']\n",
      "   ‚úÖ Tema final: ['celebrate', 'circular', 'daman', 'diu', 'festival', 'free', 'gulalai', 'haj', 'house', 'infrastructure', 'khan', 'mall', 'mcg', 'park', 'parking', 'police', 'property', 'pti', 'rakshabandhan', 'tax', 'traffic', 'transport', 'urbanism', 'women']\n",
      "\n",
      "üü© Tema 7:\n",
      "   Palabras clave iniciales: ['government', 'law', 'politics']\n",
      "   ‚ûï Palabras a√±adidas por TF-IDF: ['aaib', 'bombay', 'burqas', 'bus', 'chopper', 'crash', 'face', 'fly', 'group', 'guha', 'hair', 'hans', 'height', 'helicopter', 'low', 'norway', 'people', 'pilots', 'report', 'seats']\n",
      "   ‚úÖ Tema final: ['aaib', 'bombay', 'burqas', 'bus', 'chopper', 'crash', 'face', 'fly', 'government', 'group', 'guha', 'hair', 'hans', 'height', 'helicopter', 'law', 'low', 'norway', 'people', 'pilots', 'politics', 'report', 'seats']\n",
      "\n",
      "üü© Tema 8:\n",
      "   Palabras clave iniciales: ['crime', 'justice']\n",
      "   ‚ûï Palabras a√±adidas por TF-IDF: ['accident', 'aiims', 'also', 'anees', 'barot', 'bazmee', 'compensation', 'court', 'farmers', 'form', 'igims', 'institute', 'metre', 'negligence', 'per', 'rs', 'say', 'square', 'sunil', 'vehicle']\n",
      "   ‚úÖ Tema final: ['accident', 'aiims', 'also', 'anees', 'barot', 'bazmee', 'compensation', 'court', 'crime', 'farmers', 'form', 'igims', 'institute', 'justice', 'metre', 'negligence', 'per', 'rs', 'say', 'square', 'sunil', 'vehicle']\n",
      "\n",
      "üü© Tema 9:\n",
      "   Palabras clave iniciales: ['conflict', 'international', 'security']\n",
      "   ‚ûï Palabras a√±adidas por TF-IDF: ['air', 'campaign', 'canada', 'celebrate', 'circular', 'collude', 'daman', 'diu', 'festival', 'gujarat', 'issue', 'jet', 'kushner', 'nota', 'option', 'party', 'pilots', 'rakshabandhan', 'taxiway', 'trump']\n",
      "   ‚úÖ Tema final: ['air', 'campaign', 'canada', 'celebrate', 'circular', 'collude', 'conflict', 'daman', 'diu', 'festival', 'gujarat', 'international', 'issue', 'jet', 'kushner', 'nota', 'option', 'party', 'pilots', 'rakshabandhan', 'security', 'taxiway', 'trump']\n",
      "\n",
      "üü© Tema 10:\n",
      "   Palabras clave iniciales: ['religion']\n",
      "   ‚ûï Palabras a√±adidas por TF-IDF: ['air', 'collect', 'decision', 'dehydration', 'dilip', 'food', 'force', 'haj', 'house', 'india', 'jaitley', 'jobs', 'market', 'minuteman', 'missile', 'police', 'protect', 'say', 'test', 'vandenberg']\n",
      "   ‚úÖ Tema final: ['air', 'collect', 'decision', 'dehydration', 'dilip', 'food', 'force', 'haj', 'house', 'india', 'jaitley', 'jobs', 'market', 'minuteman', 'missile', 'police', 'protect', 'religion', 'say', 'test', 'vandenberg']\n",
      "\n",
      "üü© Tema 11:\n",
      "   Palabras clave iniciales: ['ecology']\n",
      "   ‚ûï Palabras a√±adidas por TF-IDF: ['accident', 'celebrate', 'circular', 'court', 'daman', 'diu', 'edit', 'embryos', 'festival', 'gene', 'kumar', 'negligence', 'police', 'rakshabandhan', 'say', 'scientists', 'station', 'suspect', 'vehicle', 'woman']\n",
      "   ‚úÖ Tema final: ['accident', 'celebrate', 'circular', 'court', 'daman', 'diu', 'ecology', 'edit', 'embryos', 'festival', 'gene', 'kumar', 'negligence', 'police', 'rakshabandhan', 'say', 'scientists', 'station', 'suspect', 'vehicle', 'woman']\n",
      "\n",
      "üü© Tema 12:\n",
      "   Palabras clave iniciales: ['crime', 'justice']\n",
      "   ‚ûï Palabras a√±adidas por TF-IDF: ['aide', 'bharti', 'campaign', 'collude', 'congressional', 'donald', 'dysfunctional', 'harshit', 'korea', 'kushner', 'letter', 'missile', 'north', 'offer', 'principal', 'programme', 'say', 'trump', 'united', 'war']\n",
      "   ‚úÖ Tema final: ['aide', 'bharti', 'campaign', 'collude', 'congressional', 'crime', 'donald', 'dysfunctional', 'harshit', 'justice', 'korea', 'kushner', 'letter', 'missile', 'north', 'offer', 'principal', 'programme', 'say', 'trump', 'united', 'war']\n",
      "\n",
      "üü© Tema 13:\n",
      "   Palabras clave iniciales: ['health', 'sickness']\n",
      "   ‚ûï Palabras a√±adidas por TF-IDF: ['aaib', 'bus', 'chopper', 'compensation', 'crash', 'farmers', 'fly', 'group', 'guha', 'hair', 'hans', 'helicopter', 'low', 'metre', 'norway', 'people', 'pilots', 'report', 'say', 'square']\n",
      "   ‚úÖ Tema final: ['aaib', 'bus', 'chopper', 'compensation', 'crash', 'farmers', 'fly', 'group', 'guha', 'hair', 'hans', 'health', 'helicopter', 'low', 'metre', 'norway', 'people', 'pilots', 'report', 'say', 'sickness', 'square']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "corpus = resultados\n",
    "\n",
    "# üîñ Temas base con palabras clave iniciales\n",
    "temas = [ \n",
    "    [\"child\", \"family\", \"welfare\"],\n",
    "    [\"technology\", \"science\", \"space\"],\n",
    "    [\"sports\", \"match\", \"score\"],\n",
    "    [\"opinion\", \"public\", \"speech\"],\n",
    "    [\"art\", \"culture\", \"awards\"],\n",
    "    [\"infrastructure\", \"urbanism\", \"transport\", \"traffic\"],\n",
    "    [\"politics\", \"government\", \"law\"],\n",
    "    [\"justice\", \"crime\"],\n",
    "    [\"conflict\", \"international\", \"security\"],\n",
    "    [\"religion\"],\n",
    "    [\"ecology\"],\n",
    "    [\"justice\", \"crime\"],\n",
    "    [\"health\", \"sickness\"]\n",
    "]\n",
    "\n",
    "# üìö Asignaci√≥n de documentos por tema\n",
    "asignacion_temas = {\n",
    "    0: [0, 1, 4, 13, 15],                     # G√©nero, feminismo y derechos de la mujer\n",
    "    1: [5, 6, 14, 16],                        # Violencia, crimen y justicia\n",
    "    2: [3, 16, 38, 40],                       # Terrorismo y seguridad nacional\n",
    "    3: [9, 11, 13],                           # Desinformaci√≥n, histeria colectiva y rumores\n",
    "    4: [4, 13, 20, 37],                       # Educaci√≥n y juventud\n",
    "    5: [12, 36, 39, 42],                      # Ciencia, salud y tecnolog√≠a\n",
    "    6: [7, 8, 19],                            # Protestas sociales y demandas ciudadanas\n",
    "    7: [6, 18, 22, 41],                       # Infraestructura urbana y transporte\n",
    "    8: [0, 15, 23, 35],                       # Religi√≥n y pol√≠tica identitaria\n",
    "    9: [10, 26, 28, 39, 44],                  # Medio ambiente y conservaci√≥n\n",
    "    10: [0, 5, 6, 13],                        # Corrupci√≥n y abusos de poder\n",
    "    11: [11, 35, 40],                         # Racismo, xenofobia y migraci√≥n\n",
    "    12: [7, 8, 19, 22],                       # Econom√≠a y precios\n",
    "    13: [9, 29, 30, 41],                      # Accidentes y desastres\n",
    "    14: [23, 19, 35],                         # Pol√≠tica y elecciones\n",
    "    15: [12, 13, 32, 33],                     # Tecnolog√≠a y privacidad\n",
    "    16: [1, 18, 42],                          # Cultura y celebridades\n",
    "    17: [16, 30, 34, 39],                     # Conflictos internacionales\n",
    "    18: [8, 25, 26, 41],                      # Trabajo y condiciones laborales\n",
    "    19: [5, 6, 19, 23]                        # Gobierno y ciudadan√≠a\n",
    "}\n",
    "\n",
    "\n",
    "# üîÅ Convertimos el corpus a una lista de strings para usar TF-IDF\n",
    "texts = [\" \".join(doc) for doc in corpus]\n",
    "\n",
    "# TF-IDF para todo el corpus\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Diccionario para temas expandidos\n",
    "temas_expandidos = []\n",
    "\n",
    "# üîç Para cada tema, extraer t√©rminos dominantes de sus documentos\n",
    "for idx, doc_ids in asignacion_temas.items():\n",
    "    if idx >= len(temas):  # Por si hay m√°s asignaciones que temas\n",
    "        continue\n",
    "    tema_actual = set(temas[idx])\n",
    "    \n",
    "    # Extraer submatriz TF-IDF de los documentos asignados a este tema\n",
    "    submatrix = X[doc_ids]\n",
    "    # Calcular media TF-IDF por t√©rmino en esos documentos\n",
    "    tfidf_mean = np.asarray(submatrix.mean(axis=0)).flatten()\n",
    "    # Ordenar t√©rminos por TF-IDF medio descendente\n",
    "    top_indices = tfidf_mean.argsort()[::-1][:20]  # Top 10\n",
    "    top_terms = set(terms[i] for i in top_indices if terms[i] not in tema_actual)\n",
    "    \n",
    "    # Ampliar el tema\n",
    "    tema_ampliado = tema_actual.union(top_terms)\n",
    "    temas_expandidos.append(sorted(tema_ampliado))\n",
    "\n",
    "    print(f\"\\nüü© Tema {idx+1}:\")\n",
    "    print(f\"   Palabras clave iniciales: {sorted(tema_actual)}\")\n",
    "    print(f\"   ‚ûï Palabras a√±adidas por TF-IDF: {sorted(top_terms)}\")\n",
    "    print(f\"   ‚úÖ Tema final: {sorted(tema_ampliado)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a72550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28824fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
