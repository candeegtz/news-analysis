{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ef5594-63dc-4ec7-9042-9ff2b95bf8a7",
   "metadata": {},
   "source": [
    "# Trabajo de Inteligencia artificial\n",
    " ## Análisis de noticias\n",
    "\n",
    " Realizado por:\n",
    " - Marta Aguilar Morcillo\n",
    " - Candela Jazmín Gutiérrez González\n",
    "\n",
    "Fecha: 30/05/2025\n",
    "\n",
    "Convocatoria de junio.\n",
    "\n",
    " ## 1. Lectura de datos\n",
    "\n",
    " Se comenzará con la lectura del corpus. Para ello, será necesaria la importación de las siguientes librerías:\n",
    " - **nltk:** \n",
    " - **punkt_tab:** para la tokenización de las palabras de los documentos.\n",
    " - **contractions:**\n",
    " - **sklearn:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "664d7227-c596-4ba9-9265-2ba17abe1b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\usuario\\appdata\\roaming\\python\\python39\\site-packages (3.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\usuario\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: click in c:\\users\\usuario\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\miniconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "\n",
    "from nltk import download\n",
    "\n",
    "download('punkt_tab')                           # Tokenización\n",
    "nltk.download('averaged_perceptron_tagger')     # POS tagging\n",
    "nltk.download('averaged_perceptron_tagger_eng') # POS tagging\n",
    "nltk.download('wordnet')                        # WordNet lemmatizer\n",
    "nltk.download('omw-1.4')                        # WordNet multilingüe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a763f51b-d254-49b1-9642-c4ef91d2e21f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1342639323.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [2]\u001b[1;36m\u001b[0m\n\u001b[1;33m    from nltk.corpus import wordnet as wnimport numpy as\u001b[0m\n\u001b[1;37m                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.data import path\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wnimport numpy as \n",
    "\n",
    "path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c81921b-6b6f-4e41-b7d2-ba1e3685fcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb78f2-614e-4ccb-b4de-9ed2ae89abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import re\n",
    "from bs4 import MarkupResemblesLocatorWarning\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0600f1f9-ea7b-4279-b084-e340b41f83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4734533-b325-41d9-afcc-bde22a4c36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590d2b1f-c1bd-47f5-84e9-35b82ba8e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_vacias_ingles = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1465605",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d9972-08dd-4181-9d8c-124eed854eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elimina_html(contenido):\n",
    "    return BeautifulSoup(contenido).get_text()\n",
    "\n",
    "def elimina_no_alfanumerico(contenido):\n",
    "    return [re.sub(r'[^\\w]', '', palabra)\n",
    "            for palabra in contenido\n",
    "            if re.search(r'\\w', palabra)]\n",
    "\n",
    "def expandir_constracciones(contenido):\n",
    "    return contractions.fix(contenido)\n",
    "\n",
    "def pasar_a_minuscula(contenido):\n",
    "    return contenido.lower()\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    texto = re.sub(r'[^a-zA-Z\\s]', ' ', texto)  # Reemplaza todo lo que no es letra o espacio con espacio\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    return texto\n",
    "\n",
    "def elimina_palabras_vacias(contenido):\n",
    "    return [palabra for palabra in contenido if palabra not in palabras_vacias_ingles]\n",
    "\n",
    "def lematizador(contenido):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos_tags = pos_tag(contenido)\n",
    "\n",
    "    resultado = []\n",
    "    for palabra, tag in pos_tags:\n",
    "        if tag.startswith('VB'):  # Verbos\n",
    "            resultado.append(lemmatizer.lemmatize(palabra, pos='v'))  # infinitivo\n",
    "        else:  # Sustantivos y el resto tal como están\n",
    "            resultado.append(palabra)\n",
    "\n",
    "    return resultado\n",
    "\n",
    "def extraer_noun_chunks(tokens):\n",
    "    resultados = []\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    \n",
    "    noun_chunks = [chunk.text.lower().strip() for chunk in doc.noun_chunks if len(chunk.text.split()) <= 3]\n",
    "    noun_chunks_set = set(noun_chunks)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        composed2 = \" \".join(tokens[i:i+2]).lower()\n",
    "        composed3 = \" \".join(tokens[i:i+3]).lower()\n",
    "\n",
    "        if composed3 in noun_chunks_set:\n",
    "            i += 3  \n",
    "        elif composed2 in noun_chunks_set:\n",
    "            i += 2 \n",
    "        else:\n",
    "            resultados.append(tokens[i].lower())  \n",
    "            i += 1\n",
    "\n",
    "    return resultados + noun_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be069f95-8d51-4137-82e6-c94ce64d6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proceso_contenido(texto):\n",
    "    texto = elimina_html(texto)\n",
    "    texto = expandir_constracciones(texto)\n",
    "    texto = pasar_a_minuscula(texto)\n",
    "    texto = limpiar_texto(texto)                # Limpiar antes de tokenizar\n",
    "    tokens = word_tokenize(texto)               \n",
    "    tokens = elimina_no_alfanumerico(tokens)    # Limpiar tokens individuales\n",
    "    tokens = elimina_palabras_vacias(tokens)\n",
    "    tokens = lematizador(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801b520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lectura_normalizada_corpus():\n",
    "    df = pd.read_csv(\"news_corpus.csv\", encoding=\"latin-1\", sep=\";\", quotechar='\"')\n",
    "    resultados = []\n",
    "\n",
    "    for index, fila in df.iterrows():\n",
    "        autor = [fila.iloc[0]]\n",
    "        titulo = fila.iloc[1]\n",
    "        cuerpo = fila.iloc[2]   \n",
    "\n",
    "        titulo_proc = proceso_contenido(titulo)\n",
    "        cuerpo_proc = proceso_contenido(cuerpo)\n",
    "\n",
    "        # Unir las tres listas en una sola lista combinada\n",
    "        fila_combinada =  autor + titulo_proc + cuerpo_proc\n",
    "\n",
    "        contenido_final = extraer_noun_chunks(fila_combinada)\n",
    "        resultados.append(contenido_final)\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec45a71-397d-4b8b-b0e4-4425fa107c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los primeros 3 documentos procesados\n",
    "def prueba_primeros_3_documentos_procesados(corpus):\n",
    "    for i, documento in enumerate(corpus[:3]):\n",
    "        print(f\"Documento {i+1}:\")\n",
    "        print(\" - Palabras:\", documento)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_term(term):\n",
    "    related = set()\n",
    "    for syn in wn.synsets(term):\n",
    "        for lemma in syn.lemmas():\n",
    "            word = lemma.name().replace('_', ' ').lower()\n",
    "            if word != term:\n",
    "                related.add(word)\n",
    "    return related\n",
    "\n",
    "def sinonimos_con_mismo_tfidf(palabra, tfidf, diccionario):\n",
    "    sinonimos = expand_term(palabra)\n",
    "    for p in sinonimos:\n",
    "        diccionario[p]=tfidf\n",
    "    return diccionario\n",
    "def tfidf_por_documentos_con_sinonimos(corpus_normalizado):\n",
    "    corpus = corpus_normalizado\n",
    "\n",
    "    # 🔁 Convertimos el corpus a una lista de strings para usar TF-IDF\n",
    "    texts = [\" \".join(doc) for doc in corpus]\n",
    "\n",
    "    # TF-IDF para todo el corpus\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    tfidfs_de_documentos = []\n",
    "\n",
    "    # Para cada documento se extraen sus palabras y valor tfidf\n",
    "    for idx, doc in enumerate(corpus):\n",
    "\n",
    "        submatrix_ = X[idx]  \n",
    "        tfidf_values = submatrix_.toarray().flatten()  \n",
    "        tfidf_dict = {}\n",
    "\n",
    "        # Filtrar solo los términos que están en el documento original\n",
    "        doc_terms = set(doc)\n",
    "        for i, term in enumerate(terms):\n",
    "            if term in doc_terms:\n",
    "                tfidf_valor = tfidf_values[i]\n",
    "                tfidf_dict[term] = tfidf_valor\n",
    "                tfidf_dict = sinonimos_con_mismo_tfidf(term, tfidf_valor, tfidf_dict)\n",
    "\n",
    "\n",
    "        tfidfs_de_documentos.append(tfidf_dict)\n",
    "    return tfidfs_de_documentos\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64071395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prueba_tfdifs_primeros_3_documentos(lista_diccionarios_tfidfs):\n",
    "    for idx, d in enumerate(lista_diccionarios_tfidfs[:3]):\n",
    "        top_terms = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "        print(f\"\\n🟩 Documento {idx+1}:\")\n",
    "        print(\"   ➕ Palabras añadidas por TF-IDF:\")\n",
    "        for term, score in top_terms:\n",
    "            print(f\"      - {term}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11698ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_por_documentos_sin_sinonimos(corpus_normalizado):\n",
    "    corpus = corpus_normalizado\n",
    "\n",
    "    # 🔁 Convertimos el corpus a una lista de strings para usar TF-IDF\n",
    "    texts = [\" \".join(doc) for doc in corpus]\n",
    "\n",
    "    # TF-IDF para todo el corpus\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    tfidfs_de_documentos = []\n",
    "\n",
    "    # Para cada documento se extraen sus palabras y valor tfidf\n",
    "    for idx, doc in enumerate(corpus):\n",
    "\n",
    "        submatrix_ = X[idx]  \n",
    "        tfidf_values = submatrix_.toarray().flatten()  \n",
    "        tfidf_dict = {}\n",
    "\n",
    "        # Filtrar solo los términos que están en el documento original\n",
    "        doc_terms = set(doc)\n",
    "        for i, term in enumerate(terms):\n",
    "            if term in doc_terms:\n",
    "                 tfidf_dict[term] = tfidf_values[i]\n",
    "\n",
    "        tfidfs_de_documentos.append(tfidf_dict)\n",
    "    return tfidfs_de_documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28824fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba de lectura \n",
    "resultados = lectura_normalizada_corpus()\n",
    "prueba_primeros_3_documentos_procesados(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2841c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba de tfidfs\n",
    "lista_diccionarios_tfidf = tfidf_por_documentos_sin_sinonimos(resultados)\n",
    "prueba_tfdifs_primeros_3_documentos(lista_diccionarios_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3fd6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba de tfidfs con sinonimos\n",
    "lista_diccionarios_tfidf_sinonimos = tfidf_por_documentos_con_sinonimos(resultados)\n",
    "prueba_tfdifs_primeros_3_documentos(lista_diccionarios_tfidf_sinonimos)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
